# Relaxed Synchronization
Relaxed Synchronization technique removes slow processes from the group of distributed training and prevent limiting overall training speed due to slow processes. 

In distributed learning such as data parallel, communication is required to aggregate the learning results of each process, and if there is at least one slow process, performance degradation occurs due to waiting for synchronization.
Synchronization relaxation prevents performance degradation by removing slow processes from the aggregation of training results, only using training results from remaining processes without slow processes, and continuing the training.


## 1. How to use Relaxed Synchronization

Here will be described where modification is required for your training code when using Relaxed Synchronization.
See main_amp.py for a concrete example.

### Use RelaxedSyncDistributedDataParallel from cac for DDP

#### 1) import RelaxedSyncDistributedDataParallel

    from cac.relaxed_sync import RelaxedSyncDistributedDataParallel as DDP

#### 2) Pass your model to DDP(RelaxedSyncDistributedDataParallel) and specify the parameter relaxed_sync_threshold

    model = DDP(self.model, relaxed_sync_threshold=self.relaxed_sync_threshold, relaxed_sync_mode_threshold=self.relaxed_sync_mode_threshold)

|Parameters|Features|
|------|-------|
|relaxed_sync_threshold|※1|
|relaxed_sync_mode_threshold|※2|
|simulate_slow_process|This parameter is used for comfirmation of the effect of Relaxed Synchronization. When 1 is set, artificial sleep is introduced to simulate slow processes. For the default parameter, artificial sleep is introduced from the epoch 10. |

※1：
The parameter of relaxed_sync_threshold is used to determine how many times slower the target process should be removed.
* For example, when specifying 2.0, processes which are two times slower than average processing speed are removed.

※2：
The parameter of relaxed_sync_mode_threshold determine if train_loader is recreated with rearrage_data_loaders() when the number of processes falls below the number of relaxed_sync_mode_threshold * number of initial training processes.
* For example, if you specify 0.5, when more than half of processes are removed, train_loader is recreated.

#### 3) Creation of train_loader and val_loader

Use TrainDataLoader and ValDataLoader provided by RelaxedSyncDistributedDataParallel of cac instead of the pytorch DataLoader to create train_loader and val_loader.
They are necessary for recreation of train_loader and val_loader.

    train_loader = model.TrainDataLoader(
        train_dataset, batch_size=args.batch_size, shuffle=(train_sampler is None),
        num_workers=args.workers, pin_memory=True, sampler=train_sampler, collate_fn=collate_fn)
    val_loader = model.ValDataLoader(
        val_dataset,
        batch_size=args.batch_size, shuffle=False,
        num_workers=args.workers, pin_memory=True,
        sampler=val_sampler,
        collate_fn=collate_fn)

#### 4) Calculation of loss and accuracy

Use calc_reduced_tensor() or calc_prec() from DDP of cac for calculation of loss and accuracy.
Using cac's DDP API, loss and accuracy are correctly aggregated among remaining processes even if number of remaining training processes are changed in Relaxed Synchronization.

* Aggregation over a Tensor data (Example for loss aggregation)
    ```
    reduced_loss = model.calc_reduced_tensor(loss.data)
    ```

* Aggregation over multiple Tensor data (Example for aggregation of loss, prec1 and prec5)
    ```
    reduced_loss, prec1, prec5 = model.calc_prec(loss.data, prec1, prec5)
    ```

#### 5) Call set_relaxed_pg() at the training loop

At the beginning of each Epoch, call set_relaxed_pg. This will determine if there are slow processes and remove them from the traning group.

```
model.set_relaxed_pg(epoch, min_num_processes=min_num_processes)
```
Relaxed Synchronization determines slow processes according to the relaxed_sync_threshold and a new training process group without slow processes are created.

The parameter min_num_processes determines the minimum number of processes in the new group to be generated by Relaxed Synchronization.
For example, if you specify the number of half of the initial training process, at most only half of the processes are removed, even if there are slow processes remained.

```
min_num_processes = torch.distributed.get_world_size() / 2
```

#### 6) Recreation of train_loader, val_loader

At the beginning of each Epoch, call rearrange_data_loaders to regenerate train_loader and val_loader as needed when the number of processes changes due to Relaxed Synchronization.

If the number of processes changes due to Relaxed Synchronization, val_loader is recreated based on the number of processes, and train_loader is recreated if the number of remaining processes falls below the number of relaxed_sync_mode_threshold * number of initial training processes. 

```
train_loader, val_loader = model.rearrage_data_loaders(train_loader, val_loader)
```

#### 7) Coordination of learning rate

Call adjust_lr_by_procs() when you want to coordinate learning rate (lr) when number of traning processes are changed.
The learning rate is adjusted according to the number of processes when the number of processes changes and falls below the number of relaxed_sync_mode_threshold * number of initial traning processes.

Learning Rate = Learning Rate * (Number of remaining processes / Number of initial training processes)

```
lr = model.adjust_lr_by_procs(init_lr)
```

#### 8) Finalization

Call finalize() before returning from the main function. This is necessary for notifying removed processes to exit.

```
model.finalize()
```


## 2. Environment setup
### Confirmed version for running training code and scripts

```
(py37) $ python --version
Python 3.7.10
(py37) $ pip list
Package             Version
------------------- -------------------
apex        0.1
cac-lib     0.0.1
numpy       1.21.2
olefile     0.46
Pillow      8.3.2
pip         21.2.4
setuptools  58.0.4
torch       1.6.0
torchvision 0.7.0
wheel       0.37.0
```


### Miniconda installation and setup

```
$ wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
$ bash Miniconda3-latest-Linux-x86_64.sh
$ conda config --append channels conda-forge
$ conda config --remove channels defaults
$ conda config --show channels
channels:
  - conda-forge
$ . ~/.bashrc
(base) $ conda update -n base -c defaults conda
```

### Create an environment of Python 3.7

```
(base) $ conda create -n py37 python=3.7
(base) $ vi ~/.bashrc
(add a following line at the bottom)
conda activate py37
(base) $ . ~/.bashrc
(py37) $
```


### Installation of necessary packages

* install PyTorch and torchvision

```
conda install pytorch==1.6.0 torchvision==0.7.0 cudatoolkit=10.2 -c pytorch
```


* install NVIDIA APEX

```
# Download apex
git clone https://github.com/NVIDIA/apex
# Change directory to apex
cd apex
# Install required packages 
pip install -r requirements.txt
# Install apex
pip install -v --no-cache-dir --global-option="--pyprof" --global-option="--cpp_ext" --global-option="--cuda_ext" ./
(then check “Successfully installed apex-0.1” is output at middle of the console output)
```

## Copyright  

COPYRIGHT Fujitsu Limited 2021
